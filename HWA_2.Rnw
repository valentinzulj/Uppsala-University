\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[a4paper, total={6in, 8.1in}]{geometry}
\usepackage{bm}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[font={small, it},labelfont=bf]{caption}

\title{Homework Assignment 2 -- Solutions}
\author{Valentin Zulj}
\date{}

\begin{document}

<<include = FALSE>>=
opts_chunk$set(size = "footnotesize",
  comment = NA,
  background = "#E7E7E7",
  prompt = FALSE)

library(tidyverse)
library(MASS)
library(pscl)
library(mediation)
library(broom)
library(VIM)
library(naniar)
library(gridExtra)
count <- as.tibble(read.table("count_dat.txt", header = TRUE))
count <- count %>%
  mutate(binary = ifelse(Count == 0, 0, 1))
@



\maketitle

\section{Applying and Interpreting Generalized Linear Models}
\subsection{Logit and Probit}
\subsubsection{Binary}
<<include = FALSE>>=
count <- count %>%
  mutate(binary = ifelse(Count == 0, 0, 1))

binary <- glm(binary ~ X1 + X2, family = binomial(link = "logit"), data = count)
summary(binary)

n <- 1000
a <- matrix(1, nrow = n)
b <- matrix(c(seq(-10, 10, length.out = 100)), nrow = n)
c <- matrix(mean(count$X2), nrow = n)
A <- cbind(a,b,c)

B <- matrix(c(binary$coefficients), nrow = 3)
            
probs <- c(exp(A %*% B))/(1 + c(exp(A %*% B)))

@

In this section, I run an ordinary logistic regression, using the \textit{logit} link function.The estimates, as well as the standard errors and p-values, of the logistic regression model are shown in Table \ref{tab:logit}.

\begin{table}[ht]
\centering
  \begin{tabular}{ccccc}
  \hline
   & Estimate & Std. Error & $z$-value & $p$ \\ 
  \hline
  Intercept &  0.25687 & 0.09618 & 2.67075 & 0.00757 \\ 
  $X_{1}$   & -0.60455 & 0.10345 & -5.84398 & 0.00000 \\ 
  $X_{2}$   & -0.47682 & 0.10202 & -4.67381 & 0.00000  \\ 
  \hline
  \end{tabular}
  \caption{Estimates made using logistic regression}
  \label{tab:logit}
  \end{table}
  
\noindent As can be seen $\beta_{1}$ -- which is the coefficient measuring the effect of $X_{1}$ on the response -- is strongly significant. Meaning that $X_{1}$ can be used to explain a significant share of the variability in the response variable. As for the probabilities, they can be calculated as
  \begin{align} \label{eq:probs}
  P\left(Y=1 | X_{1} = x_{1}, X_{2} = x_{2}\right) =\frac{\exp\left\{ \bm{X}_{1k} \bm{\beta}\right\}}{1   + \exp\left\{ \bm{X}_{1k} \bm{\beta}\right\}},
  \end{align}

\noindent where $\bm{X}$ is a matrix containing all covariates, and $\bm{\beta}$ is a vector containing the parameter estimates given in Table \ref{tab:logit}. In order to visualise the change in $P\left(Y=1 | X_{1} = x_{1}, X_{2} = x_{2}\right)$, I fix the value of $X_{2}$ at $\bar{x}_{2}$, and let $X_{1}$ vary on $[-10, 10]$. The different probabilities are shown in Figure \ref{fig:logit}. The figure cleary shows that the conditional probability does not change in a linear manner -- the derivative of the expression given in Equation \ref{eq:probs} is highly non-linear -- meaning that the marginal effect of $X_{1}$ will be different depending what value $X_{1}$ actually takes. In this sense, the odds ratio is a somewhat more intuitive measure, seeing as it is what the model actually assumes to be linear. The odds ratio is defined as
  \begin{align*}
  OR = \exp \left\{ \left( \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2}\right) - \left( \beta_{0} +       \beta_{1}x_{1}^{*} + \beta_{2}x_{2}\right) \right\},
  \end{align*}

\noindent where $x_{1}^{*}$ is given as the mean of $X_{1}$, while $x_{1}$ is the value found one standard deviation above that same mean. The odds ratio of this particular case is $0.539$, meaning that the odds of $Y$ being equal to 1 decrease by roughly 46 \% when $X_{1}$ increases by one standard deviation. As opposed to the probabilites, this relationship will hold for all values in the domain of $X_{1}$.

\newpage


<<logit, echo = FALSE, fig.align = "center", fig.cap = "The curve shows how the probabilities of the response variable taking the value 1 behave when $X_{1}$ is allowed to vary while $X_{2}$ is kept constant.", fig.height = 3, fig.width = 4, fig.pos = "h!">>=
ggplot(mapping = aes(x = b, y = probs)) + 
  geom_line(color = "blue") +
  labs(x = expression(X[1]),
       y = "Probability")+
  theme_classic()
@

\subsubsection{Categorical}

<<include = FALSE>>=
categorical <- polr(as.factor(Count) ~ X1 + X2, data = count, Hess = TRUE)
summary(categorical)
confint(categorical)

ctable <- coef(summary(categorical))
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
ctable <- cbind(ctable, "p value" = p)
ctable      
@

In this task, I fit an ordered-categorical model to the \texttt{count} data set, using \textit{Count} as my response variable, and both $X_{1}$ and $X_{2}$ as covariates. The model output is shown in Table \ref{tab:order-cat} and, as can be seen from the $p$-value column, the coefficient of $X_{1}$ is strongly significant, meaning that it does indeed help in explaining the the variation of the response variable. The response, however, is modeled using the \textit{logit} link, and hence the coefficient of $X_{1}$ means that a one unit increase of the covariate will correspond to a decrease of $0.023$ in the log-odds value of the response. 

Further, the funny looking coefficents are estimates of the change in intercept that is acquired when moving from one category of the response to another. They all show very strong significance, meaning that there are in fact differences in the starting levels of the different categories. Of course, each category will have such an estimate, but I only report the first few in order to save some space. The rest can be found by running the code given in the \texttt{.R} appendix file.


\begin{table}[ht]
\centering
\begin{tabular}{ccccc}
  \hline
          & Estimate & Std. Error & $t$-value & $p$ \\ 
  \hline
  X1      & -0.23041 & 0.08272 & -2.78545 & 0.00535 \\ 
  X2      & -0.25756 & 0.08410 & -3.06238 & 0.00220 \\ 
  0$|$1   & -0.19915 & 0.09259 & -2.15099 & 0.03148 \\ 
  1$|$2   & 0.52305  & 0.09545 & 5.47952  & 0.00000 \\ 
  2$|$3   & 1.16165  & 0.10609 & 10.94936 & 0.00000 \\ 
   \hline
\end{tabular}
\caption{Estimates made using the ordered categorical logit model}
\label{tab:order-cat}
\end{table}

\section{Count}
In this task, I fit a variety of regression models under the assumption that the response variable follows some Poisson distribution. The first model is an ordinary count regression model with no additives, using the logarithmic link function. The output of the model is reported in Table \ref{tab:pois}, which clearly states that $X_{1}$ cannot explain any significant share of the variation of $Y$ ($p>0.05$).

\begin{table}[ht]
\centering
\begin{tabular}{ccccc}
  \hline
  & Estimate & Std. Error & $z$-value & $p$ \\ 
  \hline
  Intercept & 0.48667 & 0.03524 & 13.81087 & 0.00000  \\ 
  $X_{1}$   & 0.05242 & 0.03435 & 1.52605  & 0.12700 \\ 
  $X_{2}$   & -0.04710 & 0.03552 & -1.32588 & 0.18488 \\ 
   \hline
\end{tabular}
\caption{Estimates made using the log-link Poisson regression}
\label{tab:pois}
\end{table}

\noindent The second model is a Poisson regression model which tries to take an assumed sense of overdispersion into account. In essence, overdispersion means that the variance of the response does not correspond to the theoretical variance suggested by the distribution assumption. In short, I assume that $Y_{i} \sim Po(\mu_{i})$, meaning that $\text{V}[Y_{i}] = \mu_{i}$. With overdispersion, however, $\text{V}[Y_{i}] > \mu_{i}$, and, because of this, the standard errors of the coefficient estimates need to be adjusted a bit. 

So I fit a model using quasi-likelihood to try and account for the alleged oversispersion, and print the output in Table \ref{tab:quasi-pois}. Comparing Tables \ref{tab:quasi-pois} and \ref{tab:pois}, it is clear that nothing has changed in the column displaying the coefficient estimates. This is a natural outcome of the quasi-likelihood approach. The standard errors, however, are somewhat higher in in Table \ref{tab:quasi-pois}, which is a result of adding a dispersion parameter to model the excess variance shown by the observed response -- $y_{i}$.

\begin{table}[ht]
\centering
\begin{tabular}{ccccc}
  \hline
  & Estimate & Std. Error & $z$-value & $p$ \\ 
  \hline
  Intercept & 0.48667 & 0.05873 & 8.28707 & 0.00000 \\ 
  $X_{1}$   & 0.05242 & 0.05724 & 0.91569 & 0.36027 \\ 
  $X_{2}$   & -0.04710 & 0.05920 & -0.79558 & 0.42665 \\ 
   \hline
\end{tabular}
\caption{Estimates made using quasi-likelihood to control the overdispersion}
\label{tab:quasi-pois}
\end{table}

\noindent Neither one of the above mentioned count models result in coefficient estimates that are actually significant, and hence, there is room for some extra troubleshooting. 

Modelling count data, there is always a chance that a great part of the observations are clustered by some particular value of the response -- usually $0$. Plotting $X_{1}$ against the repsonse variable \textit{Count} yields Figure \ref{fig:infl}. The figure shows that observations for all values of $X_{1}$ seem to cluster around 0, suggesting that it might be suitable to use two different models for the data set at hand, with one being binary -- determining whether an observation belings to the \textit{zero class} or not -- and the other being an ordinary count regression. This way of modelling produces what is called a zero-inlfated model, and the output of my attempt at making one is shown in Table \ref{tab:z-count}.

\begin{table}[ht]
\centering
\begin{tabular}{ccccccccc}
  \hline
  & \multicolumn{4}{ c }{\textbf{Count Model}} & \multicolumn{4}{ c }{\textbf{Zero Model}}\\
  \cmidrule(lr){2-5}
  \cmidrule(lr){6-9}
  & Estimate & Std. Error & $z$-value & $p$ & Estimate & Std. Error & $z$-value & $p$ \\ 
  \hline
  Intercept & 1.06 & 0.04 & 27.91 & 0.00 & -0.95 & 0.17 & -5.67 & 0.00 \\ 
  $X_{1}$   & 0.55 & 0.04 & 13.75 & 0.00 & 1.63 & 0.22 & 7.53 & 0.00\\ 
  $X_{2}$   & 0.31 & 0.04 & 7.41 & 0.00  & 1.05 & 0.17 & 6.14 & 0.00\\ 
   \hline
\end{tabular}
\caption{Estimates of the zero-inflated count model}
\label{tab:z-count}
\end{table}

\newpage

<<infl, echo = FALSE, fig.align = "center", fig.cap = "The figure illustrates the number of observations found in each category of the respobse variable", fig.height = 3, fig.width = 4, fig.pos = "h!">>=
count %>%
  ggplot(mapping = aes(x = Count, y = X1)) +
  geom_point(mapping = aes(alpha = 3), color = "blue") + 
  labs(y = expression(X[1])) + 
  guides(alpha = FALSE) +
  theme_classic()
@

Looking at the table, it is clear that the coefficient of $X_{1}$ is significantly positive in both models. The count model states that higher values of $X_{1}$ tend to give higher values of the response variable. The zero model models the log-odds that each response follows a degenerate distribution at zero. Hence, plugging the estimates into Equation \ref{eq:probs}, we find the conditional probability that observation $i$ will always have a response of value 0. Again, I fix the value of $X_{2}$ at $\bar{x}_{2}$, and evaluate Equation \ref{eq:probs} using 
    \begin{align*}
      \bm{X}_{1} = \left(1, \; \bar{x}_{1}, \; \bar{x}_{2} \right) \ \text{and} \ \bm{X}_{2} =          \left(1, \; \bar{x}_{1} + \hat{\sigma}_{X_{1}}, \; \bar{x}_{2} \right),
    \end{align*}
    
\noindent where $\hat{\sigma}_{X_{1}}$ is the sample standard deviation of $X_{1}$. Doing this, the probabilities are calculated to be
    \begin{align*} \label{eq:probs}
        P\left(Y=1 | X_{1} = \bar{x}_{1}, X_{2} = \bar{x}_{2}\right)  = 0.2648 \ \text{ and } \
        P\left(Y=1 | X_{1} = \bar{x}_{1} + \hat{\sigma}_{x_{1}}, X_{2} = \bar{x}_{2}\right)= 0.6553.
    \end{align*}

\noindent Hence, the probability of $Y_{i}$ being an 'absolute zero' increases markedly as $X_{1}$ changes from $\bar{x}_{1}$ to $\bar{x}_{1} + \hat{\sigma}_{X_{1}}$. This rate of change, however, will not be constant, and moving one standard deviation from an arbitrary value of $X_{1}$ will not yield the same result.




\section{Moderation and Mediation}
<<include = FALSE>>=
med <- as.tibble(read.table("med_dat.txt", header = TRUE))

moderator <- lm(perform ~ negexp*negtone + dysfunc, data = med)

basic <-  lm(perform ~ negexp + negtone + dysfunc, data = med)
n <- seq(min(med$negtone), max(med$negtone), length.out = 100)
y1 <- matrix(mean(med$negexp), nrow = 100)
y2 <- matrix(n, nrow = 100)
y3 <- matrix(mean(med$dysfunc), nrow = 100)
y4 <- matrix(y1*y2, nrow = 100)
Y <- cbind(1,y1,y2,y3,y4)

b1 <- matrix(basic$coefficients, nrow = 4)
b2 <- matrix(moderator$coefficients, nrow = 5)

u <- Y[,1:4] %*% b1
v <- Y %*% b2
@


\subsection{Moderation}
In this section, I fit a moderation model studying the working performance of a given member of a work-team. The model I want to fit strives to determine the performance of a member using a certain set of covariates called \textit{negtone, negexp} and \textit{dysfunc}. Moreover, I want \textit{negexp} to moderate the effect of \textit{negtone} on the response variable, \textit{perform}. In doing so I use
  \begin{align*}
  \text{perform} = \beta_{0} + \beta_{1} \text{dysfunc} + \beta_{2}   \text{negtone} + \beta_{3} \text{negexp} + \beta_{4} \text{negexp}   \cdot \text{negtone}
  \end{align*}
  
\noindent to model the relationship between the response and the covariates, meaning that the interaction term -- $ \text{negexp}   \cdot \text{negtone} $ -- will be the moderator. Fitting the above mentioned model to the data set yields the results shown in Table \ref{tab:mod}.


\begin{table}
\centering
\begin{tabular}{ccccc}
  \hline
  & Estimate & Std. Error & $z$-value & $p$ \\ 
  \hline
  Intercept   & -0.0119 & 0.0585 & -0.20 & 0.8399 \\ 
  negexp      & -0.0192 & 0.1174 & -0.16 & 0.8708 \\ 
  negtone     & -0.4357 & 0.1306 & -3.34 & 0.0015 \\ 
  dysfunc     &  0.3661 & 0.1778 & 2.06 & 0.0443 \\ 
  Interaction & -0.5170 & 0.2409 & -2.15 & 0.0363 \\ 
   \hline
\end{tabular}
\caption{Regression model estimates, the effect of \textit{negtone} on \textit{perform} is moderated by \textit{negexp}}
\label{tab:mod}
\end{table}


\noindent The concept of using a moderator in linear modelling is perhaps best explained through visualisation. Hence, I create a plot visualising the effect of moderation. I fix the values of \textit{negexp} and \textit{dysfunc} at their respective means, and let \textit{negtone} vary on $\left[\min\left(\textit{negtone}\right), \max\left(\textit{negtone}\right) \right]$. Then, I fit a regression model without moderation and compute the fitted values using the coefficient estimates of both models. I then plot the fitted values in the same figure in order to compare the outcomes. The results are shown in Figure \ref{fig:mod}.

<<mod, echo = FALSE, fig.align = "center", fig.cap = "The figure illustrates the number of observations found in each category of the respobse variable", fig.height = 3, fig.width = 7, fig.pos = "h!">>=
ggplot() +
  geom_line(mapping = aes(x = n, y = c(u), color = "Ordinary")) +
  geom_line(mapping = aes(x = n, y = c(v), color = "Moderated")) +
  labs(y = "Fitted Value", x = "Negtone") +
  theme_classic()
@

\noindent As can be seen, in this case the moderator dampens the effect of \textit{negtone} on \textit{perform}, an effect which can easily be shown using a little bit of mathematics. For the sake of simplicity, consider
  \begin{align*}
    y_{i} = \beta_{1} x_{1} + \beta_{2} x_{2} +\beta_{3} x_{1} x_{2},
  \end{align*}

\noindent and let $x_{2}$ equal some constant $c$ -- in my case $c = \bar{x}_{2}$. Then, the marginal effect of $x_{1}$ can be found through the fact that the linear equation can be rewritten as
  \begin{align*}
    y_{i} = \beta_{2}c + \left( \beta_{3}c + \beta_{1} \right) x_{1},
  \end{align*}

\noindent where $\beta_{3}c + \beta_{1}$ is the effect of $x_{1}$ on the response.

In that sense, the effect of \textit{negtone} can not be viewed only in terms of what \textit{negtone} does alone, but its interaction with \textit{negexp} also needs to be accounted for in order to find out how \textit{negtone} truly affects \textit{perform}. Table \ref{tab:mod} states that \textit{negtone} significantly affects \textit{perform} both directly and through the interaction term, meaning that the total effect is a combination of the two.


\subsection{Mediation}
In the mediation model, I want to examine the effect of \textit{dysfunc} on \textit{perform}. Obviously, the most intuitive way to do this is using linear regression. However, I also want to determine whether \textit{dysfunc} affects \textit{negtone} in a way which spills over to \textit{perform}. Hence, I use a set of linear equations, defined as
  \begin{equation}
  \left.\begin{aligned}
    &\text{negtone} = \gamma_{0} + \gamma_{1} \cdot \text{dysfunc} \\
    &\text{perform} = \beta_{0} + \beta_{1} \cdot \text{negtone} + \beta_{2} \cdot \text{dysfunc}
  \end{aligned} \right\},
  \end{equation}

\noindent where the upper equation controls the mediator effect, and the second is just the ordinary linear model. I run the \texttt{mediate} function using linear models based on the equations, and get the results displayed in Table \ref{tab:med}. The table shows that \textit{dysfunc} affects the response of observation $i$ significantly in two ways, both directly and also through the effect is has on \textit{negtone}, which leads to an indirect effect on the response. 

However, the total effect -- or the combined effect of \textit{dysfunc} on \textit{perform} -- is not significant. This is rather intuitive, if the sign of the two effects are taken into account. Seeing as the mediated effect is negative, and the direct effect is positive, and since they are quite similar in size, they may well cancel each other out. This seems to be the case, and since it is so, the total effect of \textit{dysfunc} can not be shown to be significant.

\begin{table}[ht]
\centering
\begin{tabular}{ccccc}
  \hline
  & Estimate & Lower & Upper & $p$ \\ 
  \hline
    ACME           & -0.324  & -0.568   & -0.12  & $<$2e-16  \\ 
    ADE            &  0.443  &  0.110   &  0.79  & 0.006   \\ 
    Total Effect   &  0.119  & -0.224   &  0.48  & 0.528   \\ 
    Prop. Mediated & -1.080  & -22.247  &  19.30 & 0.528  \\
   \hline
\end{tabular}
\caption{Estimates made using the mediator model. The columns named 'lower' and 'upper' give limits of the confidence intervals made for each estimate. ACME gives the average mediated effect, and ADE gives the average direct effect.}
\label{tab:med}
\end{table}


\subsection{The Classic Paper}
The paper seems to be focused on highlighting the differences between mediating and moderating effects within statistical analysis, and manages to do that quite simply and intuitively. In essence, I would say that a mediating variable induces some other variable to have an effect on the response, while a moderator manages the strength and direction of the effect that some variable has on the response.

Consider the act of throwing food in the trash can, and think of the amount of food waste produced as a response variable. Now, imagine that you are about to enter your local supermarket to do your weekly grocery shopping. Of course, the amount of food you can throw away is governed by the amount of food that you buy. Further, think of a binary variable taking value 1 if you are hungry, and 0 of you are not. People often say that you shouldn't go shopping for food if you are hungry, since this will lead you to buy things that you have no use of. In this sense, hunger could be seen as a mediating variable, since it is thought to affect the way in which you behave at the store, which could eventually impact how much food you throw away.

A moderating effect, on the other hand, could be found by studying your schedule. If you are incredibly busy, and you have lots of extracurricular activities going on, you might not have time to make use of all the food you have bought. In that sense, 'busyness' can be thought to moderate how much of the food you buy that will eventually end up in the bin.

\section{Missing Data and Multiple Imputation}

\subsection{Applying \texttt{mice} to Generalized Linear Models}

<<include = FALSE>>=
count_missing <- as.tibble(read.table("count_dat_missing.txt", header = TRUE))

na_count <- count_missing[!complete.cases(count_missing), ]
ok_count <- count_missing[complete.cases(count_missing), ]

na_outcome <- ggplot(data = na_count, mapping = aes(x = Count)) +
  geom_histogram(binwidth = 1, fill = "white", color = "black") + 
  labs(title = "Missing", y = NULL) +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"))

ok_outcome <- ggplot(data = ok_count, mapping = aes(x = Count)) +
  geom_histogram(binwidth = 1, fill = "white", color = "black") + 
  labs(y = "Frequency", title = "Full") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"))

na_x2 <- ggplot(data = na_count, mapping = 
                  aes(x = Count, y = X2))+
  geom_point() +
  labs(x = "Count", y = NULL) +
  theme_classic()

ok_x2 <- ggplot(data = ok_count, mapping = 
                  aes(x = Count, y = X2))+
  geom_point() +
  labs(y = expression(X[2]), x = "Count") +
  theme_classic()

@

In this task, I work with the \texttt{SEXDISC} data set. The data consists of four different variables, one of which contains missing values. In order to determine the extent -- and the type -- of the missingness, I make a series of plots that show how the \texttt{NA} terms are distributed across the variables. The illustration of missingness is displayed in Figure \ref{fig:missing}.

<<missing, echo = FALSE, fig.align = "center", fig.cap = "Plots illustrating the missingness in the data set", message = FALSE, fig.height = 4, fig.pos = "h!">>=
grid.arrange(ok_outcome, na_outcome, ok_x2, na_x2, ncol = 2)
@

\noindent The data set conatins missing values in the $X_{1}$ variable, so I divide the data set into two, on where all values of $X_{1}$ are observed, and one where they are missing. Hence, the left hand side of Figure \ref{fig:missing} shows the data set where all variables have been observed, and the right hand side shows the subset where $X_{1}$ is missing.

Looking at the figure, there are two things that come to mind. First, the missingness does not seem to have a great impact on the distribution of the outcome -- if any at all. In the first row, the response -- \textit{Count} -- has been gathered into histograms. It seems the distribution is roughly the same, with the frequency decreasing exponentially as the count increases. This indicates that the values of $X_{1}$ might be missing at random.

In order to handle data that is missing at random, there has to be a relationship between the missingness and one of the other covariates, so that the latter can be use to model the missing values. The second row of the figure suggests that might be the case, since the missing values seem to occur predominantly at higher values of $X_{2}$. Further, the second row also suggests that there is no relationship between the outcome and the missingness, since the missingness seems to be relatively spread out across the \textit{Count} variable. Because of this, I would say that the values of $X_{1}$ are likely to be missing at random.

To handle the missingness, I use the \texttt{mice} package in order to impute the missing data points, creating 5 different imputed data sets. After that, I fit Poisson regression models to each data set, and combine the estimates of each model through pooling. The outcome of the modelling is shown in Table \ref{tab:imp}. As can be seen, $X_{1}$ is not very good for explaining any share of the variation in the response, seeing as its coefficient is not significant -- and very far from being so at all.

\begin{table}[ht]
\centering
\begin{tabular}{cccccc}
  \hline
  & Estimate & Std. Error & Statistic & df & $p$ \\ 
  \hline
  Intercept &  0.49088 & 0.03528 & 13.91271 & 494.50098 & 0.00000 \\ 
  $X_{1}$   &  0.03671 & 0.03983 & 0.92184 & 127.41301 & 0.35706  \\ 
  $X_{2}$   &  -0.05345 & 0.03662 & -1.45974 & 488.58911 & 0.14500  \\ 
  \hline
\end{tabular}
\caption{Table of estimates made using Poisson regression on the imputed data sets}
\label{tab:imp}
\end{table}

\section{Multiple Imputation for a Regression Model}
In this section, I will use another data set containing missing values, impute the missingness, and fit regression models to the imputed data sets. However, I will write my own function to fit the linear regressions and pool the variances, and not use the \texttt{mice} package. I write a function called \texttt{rubin}, and use it to compute the estimates given in Table \ref{tab:rub}. The contents of the table are identical to the output renderd using \texttt{mice} -- as in the previous task.

\begin{table}[ht]
\centering
\begin{tabular}{ccc}
  \hline
 & Estimate & Std. Error \\ 
  \hline
  Intercept & 3.18923 & 0.64369 \\ 
  Sexism & 0.12187 & 0.10543 \\ 
  Approp & 0.39010 & 0.08057 \\ 
  Protest & -0.15473 & 0.21849 \\ 
   \hline
\end{tabular}
\caption{Estimates found using my own \texttt{rubin} function}
\label{tab:rub}
\end{table}

\noindent In order to acquire proper estimates of the standard errors, I need to pool them, rather than just use their mean. This is because imputing the missing data points means adding an extra degree of uncertainty into the estimation procedure and inference. Hence, in order to account for the extra uncertainty, the variances need to be pooled so that there is no systematic error in the inference made using the imputed data set.

\section{A Classification of Data Science Tasks}

I like to think of statistical modelling in terms of what questions I want to answer using the statistical analysis I am about to perform, because I believe that the distinction between prediction and inference significantly reduces the number of ways in which I can approach any given task. Also, I believe that an important question to ask is whether I -- using only my statistics training an no other input -- can give a meaningful answer to the question at hand.

Think of it like this: a stock broker walks into your office and asks you what price a share of company so-and-so will have exactly a week from now. Can you answer that question? Sure. Or at least answer-ish. You simply build a model using data regarding the previous prices of the stock, perhaps you use the interest rates or oil prices or whatever, and so you can make a qualified guess. Point being it does not really matter how you do it, as long as you make a good enough prediction, the stock broker will happily give you a share of his profits.

Now, suppose you want to determine what effect interpersonal trust has on voter turnout during election day. Could you answer that question all by yourself, given that you have only a data set and no other knowledge? Probably not. You would need to consult a political scientist, and perhaps somebody working in sociology or psychology. Using data to make statements regarding causal relationships is a complex issue, and there always has to be some sort of motivation as to why a variable is controlled for, or excluded or deemed irrelevant. That, I would say, is the main difference between predictive and inferential modelling.

With reference to the discussion above, I would say that it is \textit{always} important to distinguish between prediction and inference, seeing as they are two separate forms of statistical analysis. From that particular point of view, I think this course has been really interesting. Looking at the table shown at the very end of the paper, I see that regression has been given as an analytic tool that can be used for both inference \textit{and} predictions. Thinking about that for a while, I end right where I left of: what sort of question do I want to answer? Or perhaps something more heuristic -- am I interested in $\hat{\bm{\beta}}$ or $\hat{\bm{y}}$? For the former, causal inference is probably the thing, and for the latter, predictive modelling is likely to be the most suitable approach.

\end{document}
