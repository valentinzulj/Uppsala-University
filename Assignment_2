library(tidyverse)
library(glmnet)
library(broom)
library(modelr)
library(parallel)
library(microbenchmark)

load("prostate.RData")
prostate <- as.tibble(prostate)

house <- as.tibble(read.csv("house_prices.csv"))
train <- as.tibble(house %>%
          filter(train == TRUE))
test <- as.tibble(house  %>%
          filter(train == FALSE))

#### Ridge regression ####
indep <- prostate %>%               # Extracting covariate names
  select(-c(lpsa, group)) %>%
  colnames()
dep <- "lpsa"

ridge <- function(data, dep, indep, lambda){  # Estimating betas
  int <- matrix(rep(1, nrow(data)), ncol = 1) # Intercept ones
  reg <- as.matrix(data %>%           # Indepentent vars
                    select(indep)) 
  X <- cbind(int, reg)                # Regressor matrix
  y <- as.matrix(data %>%             # Dependent variable
                   select(dep))
  lambda <- lambda
  Id <- diag(ncol(X))                 # Identity matrix
  beta_ridge <- solve(t(X)%*%X + lambda*Id) %*% t(X) %*% y
  beta_ridge = as.vector(beta_ridge)  # Vectorizing output
  return(beta_ridge)
}

beta_hat <- ridge(prostate, dep,    # Saving betas
                  indep , 10)
beta_hat
class(beta_hat)                     # Testing class

pred <- function(data, indep, beta_hat){      # Estimating yhats
  int <- matrix(rep(1, nrow(data)), ncol = 1) # Intercept ones
  reg <- as.matrix(data %>%           # Indepentent vars
                     select(indep)) 
  X <- cbind(int, reg)                # Regressor matrix
  B <- as.matrix(beta_hat)            # Betas as matrix
  preds <- X %*% B                    # Predicted values
  preds <- as.vector(preds)
  return(preds)
}

pred_vals <- pred(prostate, indep, beta_hat)
pred_vals <- tibble(Predicted = pred_vals)
pred_vals

cv <- function(data, dep, indep, lambda){
  n <- length(unique(data$group))    # Number of groups
  mse <- numeric(n)                  # Preallocation
  for (i in 1:n){                    # One per group
    X <- data %>%                    # Filtering groups
      filter(group != i)
    X_pred <- data %>%
      filter(group == i)             # Data for prediction
    Y <- X_pred %>% 
      pull(dep)                      # Values to predict
    betas <- ridge(X, dep = dep,     # Dependent var
                   indep = indep,    # Independent vars
                   lambda = lambda)
    preds <- pred(X_pred, indep, 
                  betas)             # Using estimated betas
    mse[i] <- mean((Y - preds)^2)    # Computing MSE
    
  }
  mse <- as.vector(mean(mse))        # Mean of MSE:s
  return(mse)
}


cv(prostate, dep, indep, 10)


l <- seq(0, 50, length.out = 50)
mse <- numeric(length(l))
for(i in 1:length(l)){
  mse[i] <- cv(prostate, "lpsa", indep, l[i])

}
mse

mse %>%
  enframe() %>%
  ggplot() +
  geom_line(aes(x = name, y = value, color = "red")) +
  labs(x = expression(lambda),
       y = "MSE",
       title = expression(MSE~plotted~against~lambda)) +
  theme_classic() +
  theme(plot.title = element_text(face = "bold",              
                                  hjust = 0.5, size = 15),
        axis.title.x = element_text(size = 15),
        panel.grid.major = element_line(color = "gray90")
        ) + 
  guides(color = FALSE)
  
#### Bagging ####
bag <- function(B, train, test){
  rmse <- numeric(B)  # Preallocation
  
  for(i in 1:B){
    sample <- sample_n(train, 1465, replace = TRUE) # Sample from sample
    mod <- lm(SalePrice ~ ., data = sample)         # Full model
    mod_sel <- step(mod, trace = FALSE)             # Model selection
    mod_step <- lm(formula(mod_sel), data = sample) # Reduced model
    
    options(warn = -1)                                  # Kills warining message
    rmse[i] <- test %>%
      add_predictions(mod_step)    %>%                  # Predictions
      mutate(rmse = sqrt(mean(SalePrice - pred)^2)) %>% # RMSE
      summarize(rmse = mean(rmse)) %>%                  # Mean of RMSE
      pull(rmse)                                        # Extracting RMSE
    options(warn = 1)
  }
  out <- mean(rmse) # Mean RMSE of B reps
  return(out)
}

cl <- makeCluster(4)
clusterEvalQ(cl, {
             library(tidyverse)
             library(modelr)
             })
clusterExport(cl, varlist = c("train", "test"))

parallel <- parSapply(cl, rep(250,4), FUN = bag, 
                      train = train, test = test)  # Parallel computing
parallel <- mean(parallel)                         # Final value
parallel


#### Feature Bagging ####
feature_bag <- function(B, train, test){
  B <- B
  train <- train
  test <- test
  workers <- makeCluster(4)                     # Setting up workers
  clusterEvalQ(workers, {
    library(tidyverse)
    library(modelr)
  })
  f_bag <- function(B, train, test){
    rmse <- numeric(B) # Preallocation
    covariates <- train %>%
      select(-c(SalePrice, train)) %>%
      colnames()                            # Covariates to sample from
    dep <- train %>%                   
      select(SalePrice) %>%
      colnames                              # Dependent variable
    sqp <- round(sqrt(length(covariates)))  # Covariate sample size
    
    
    for(i in 1:B){
      sample <- sample_n(train, 1465, 
                         replace = TRUE)      # Sample
      sample_covariates <- sample(covariates, sqp, replace = FALSE)           # Sample covariates
      formula <- paste(dep, '~', paste(sample_covariates, collapse = ' + ' )) # Regression formula
      mod <- lm(formula, data = sample)       # Regression model
      
      options(warn = -1)                                   # Kills warining message
      rmse[i] <- test %>%
        add_predictions(mod)         %>%                   # Predictions
        mutate(rmse = sqrt(mean(SalePrice - pred)^2)) %>%  # RMSE
        summarize(rmse = mean(rmse)) %>%                   # Mean of RMSE
        pull(rmse)                                         # Extracting RMSE
      options(warn = 1)
    }
    obj <- mean(rmse)
    return(obj)
  }        # Inlcuding function
  parallel <- parSapply(workers, rep((B/4), 4),
                        FUN = f_bag, train = train,
                        test = test)            # Parallel computing
  out <- mean(parallel)                         # Mean of parallel computations
  return(out)
}




feature_bag(100, train, test)

f_bag <- function(B, train, test){
  rmse <- numeric(B) # Preallocation
  covariates <- train %>%
    select(-c(SalePrice, train)) %>%
    colnames()                            # Covariates to sample from
  dep <- train %>%                   
    select(SalePrice) %>%
    colnames                              # Dependent variable
  sqp <- round(sqrt(length(covariates)))  # Covariate sample size
  
  
  for(i in 1:B){
    sample <- sample_n(train, 1465, 
                       replace = TRUE)      # Sample
    sample_covariates <- sample(covariates, sqp, replace = FALSE)           # Sample covariates
    formula <- paste(dep, '~', paste(sample_covariates, collapse = ' + ' )) # Regression formula
    mod <- lm(formula, data = sample)       # Regression model
    
    options(warn = -1)                                   # Kills warining message
    rmse[i] <- test %>%
      add_predictions(mod)         %>%                   # Predictions
      mutate(rmse = sqrt(mean(SalePrice - pred)^2)) %>%  # RMSE
      summarize(rmse = mean(rmse)) %>%                   # Mean of RMSE
      pull(rmse)                                         # Extracting RMSE
    options(warn = 1)
  }
  obj <- mean(rmse)
  return(obj)
}

f_bag(100, train, test)


microbenchmark(utv = feature_bag(100, train, test),
               enkel = f_bag(100, train, test))
