\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[a4paper, total={6.1in, 8.1in}]{geometry}
\def \lreg{\texttt{linear\_regression}}


\author{Valentin Zulj \&  Vilgot \"{O}sterlund}
  \title{Solutions to Assignment 1}
  \date{September 30, 2018}
  
  
  \begin{document}
  \maketitle
  <<include = FALSE>>=
  linear_regression <- function(data, dep, indep, intercept = TRUE) {
  y <- as.matrix(data[, dep])
  x <- as.matrix(data[, indep])
  if (intercept == TRUE) { x <- cbind(1, x)
  }
  beta <- c(solve(crossprod(x)) %*% crossprod(x, y))
  fits <- x %*% beta
  resids <- y - fits
  sigma2 <- sum(resids^2)/(length(resids)-ncol(x))
  se <- sqrt(diag(sigma2 * solve(crossprod(x))))
  names(beta) <- colnames(x)
  return_obj <- list(beta = beta, se = se,
  residuals = c(resids), fitted = c(fits),
  sigma = sqrt(sigma2), dep = dep, indep = indep,
  intercept = intercept, y = c(y))
  class(return_obj) <- "linear_regression"
  return(return_obj)
  }
  library(xtable)
  library(tidyverse)
  library(ggrepel)
  library(devtools)
  library(scales)
  
  crime_data <- read_tsv("crime_data.tsv")
  dictionary_county_facts <- read_csv("county_facts_dictionary.csv")
  county_facts <- read_csv("county_facts.csv")
  results <- read_csv("general_result.csv")
  
  results <- results %>%
  arrange(combined_fips) # Sorterar datatt efter fips i storleksordning, minst f??rst
  
  colnames(results)[2] <- "fips" # Byter namn s?? att alla ska va samma
  
  county_facts <- na.omit(county_facts, cols = "state_abbreviation") # Tar bort de rader som inneh??ller stat- eller landdata
  colnames(county_facts)[1] <- "fips" # Byter namn s?? att alla ska va samma
  county_facts$fips <- as.character(county_facts$fips) # F??r att kunna joina m??ste fips vara av samma class
  results$fips <- as.character(results$fips)
  
  results_county <- full_join(county_facts, results) # L??gger samman county_facts och results, matchar med fips
  
  for (i in 1:nrow(crime_data)) {
  if(crime_data[i, 6] < 10){ # Om county-koden ??r mindre ??n 10, multipliceras stat-koden med 100
  crime_data[i, 5] <- crime_data[i, 5]*100
  }else{
  if(crime_data[i, 6] < 100 & crime_data[i, 6] > 9){ # Om county-koden ??r st??rre ??n 10 men mindre ??n 100, multipliceras stat-koden med 10
  crime_data[i, 5] <- crime_data[i, 5]*10
  }
  }
  }
  
  crime_data <- unite(crime_data, FIPS_ST, FIPS_CTY, col = "fips", sep = "") # L??gger ihop stat-koden och county-koden till en gemensam fipskod
  sammanslaget <- full_join(results_county, crime_data)  # L??gger samman results_county och crime_data, matchar med fips
  
  sammanslaget <- sammanslaget %>%
  select(fips, per_gop_2016, total_votes_2016, everything()) # Flyttar fram kolumner av intresse
  
  
  sammanslaget <- na.omit(sammanslaget, cols = "per_gop_2016") # Tar bort rader med NA p?? variabeln andel av r??sterna f??r republikanerna
  
  sammanslaget <- sammanslaget %>% # L??gger till en ny kolumn i datamaterialet, "andel av r??sterna f??r republikanerna" - "andel av r??sterna f??r demokraterna"
  mutate(gop_win = per_gop_2016 - per_dem_2016) %>%
  select(gop_win, everything())
  
  for (i in 1:nrow(sammanslaget)) {
  if(sammanslaget[i, 1] > 0){ # Om gap_win ??r st??rre ??n 0, kodas gap_win som 1, annars som 0. 1 = republikansk seger, 0 = demokratisk seger 
  sammanslaget[i, 1] <- 1
  }else{
  sammanslaget[i, 1] <- 0
  }
  }
  
  
  sammanslaget$gop_win <- as.factor(sammanslaget$gop_win) # ??ndrar gap_wins class till factor
  
  opts_chunk$set(size = "footnotesize",
  comment = NA,
  background = "#E7E7E7",
  prompt = FALSE)

@
  
  \section{Linear Regression} \label{sec:reg}
In this first task we were given a funtction, \texttt{linear\_regression}, computing OLS estimates of linear regression parameters, and asked to write a function which gives the confidence interval for any given parameter.

We begin by simulating a random set of data to run through the functions:
  
<<>>=
A <- data.frame(Y = rexp(100, rate = 2),          # Depentent variable
               X1 = rnorm(100),                   # Independent variable
               X2 = rnorm(100, mean = 4, sd = 2)) # Independent variable

@
  \noindent And then proceed to estimate regression parameters, assinging the results to a new object:
  
<<>>=
lin_mod <- linear_regression(data = A, dep = 1, # y is the first column of 'data'
                            indep = c(2,3))     # Regressors are in columns 2 and 3
@
  
\noindent Now, the \lreg { } function returns estimates of regression parameters and their standard errors, which are extracted from the object \texttt{lin\_mod} as follows:
  
<<>>=
lin_mod$beta
lin_mod$se
@
  
Knowing that, we begin construction of our own function set to return a confidence interval.

\subsection{Function}
Our interval function takes three arguments, namely \texttt{lin\_mod}, \texttt{pos}, and \texttt{alfa}. \texttt{lin\_mod} consits of the regression output given by \lreg, and \texttt{alfa} is simply the singificance level. The \texttt{pos} argument denotes the position of the parameter for which we want to estimate the interval, meaning that if the linear regression model contains an intercept \texttt{pos = 1}, will yield an interval for the intercept. This is perhaps best illustrated by the code shown at the very end of the last section. In code, the interval looks like:

<<>>=
ci <- function(lin_mod, pos, alfa){
  i <- pos                              # Determines which parameter to estimate
  beta <- lin_mod$beta[i]               # Extracting coefficients
  se <- lin_mod$se[i]                   # Extracting standard errors
  lower <- beta - qnorm(1-(alfa/2))*se  # qnorm() gives standard normal quantiles
  upper <- beta + qnorm(1-(alfa/2))*se
  out <- list(lower = lower,
              upper = upper,
              c_level = 100*(1-alfa),
              var = pos)                # Objects to use in output
  class(out) <- "linear_regression_ci"  # Creating a new class for output
  return(out)}
@
\noindent As is stated, the intervals are calculated using the quantiles of the standard normal distribution, this, of course, being a result of the central limit theorem.

In our function, we create a new class and assign it to the output variable. We do this in order to edit the message printed when \texttt{print()} is called on our function. The editing of the function output will be described in Section~\ref{sec:print}.

\subsection{Printing Method} \label{sec:print}
As mentioned above, we will now edit the printing method connected to our \texttt{ci} function, seeing as we want it to print a message stating what parameter has been used, as well as the confidence level and, of course, the limits of the interval itself. To do this we make use of the fact that \texttt{print()} is a generic function, and that it can easily be edited in order to modify the output given when calling on it. In short, we edit the \texttt{print()} call of the new class given to the output object of our function, all  int accordance with the following code:
  
<<>>=
print.linear_regression_ci <- function(obj){
    print(paste0("A ", obj$c_level,                   # The confidence level of the interval
                 "% confidence interval for beta_",
                 obj$var,                             # Shows which parameter we use
                 " is given by: (",
                 round(obj$lower, digits = 3),        # Rounds lower limit to 3 decimals
                 ",",
                 round(obj$upper, digits = 3),        # Rounds upper limit to 3 decimals
                 ")."))}
@
\noindent Now, running the \texttt{print()} function on an object created using our \texttt{ci()} function will result in the following output:
  
<<>>==
int <- ci(lin_mod, pos = 1, alfa = 0.05)
int
@
  
\noindent Note that in this case, $\hat{\beta}_{1}$ should be interpreted as the the intercept of the regression model.

\section{A Two-sample t-test for Stratified Data}

In this task, we were asked to write a function that computes t-tests, and can do so for data that is either stratified or not. We begin by demonstrating the way in which we wrote the code used for stratified data, then move on the ordinary t-test, and finally we combine them into a \texttt{t\_test()} function.

\subsection{Stratified Data} \label{sec:strat}
We begin by sampling a set of stratified data to be used in our calculations:
<<>>=
set.seed(2018)
strat <- tibble(x = c(rnorm(200, 25), rnorm(200, 45), rnorm(200, 75)),
                treatment = rep(1:2, 300),
                strata = c(rep(1, 200), rep(2, 200), rep(3, 200)))

@
  
\noindent Having done that, we notice there is a need to subset the data, so that we can calculate one set of the needed variables for each treatment group. First, we group our data by treatment and then by stratum, and calculate the values of $\bar{x}$, $n$, and $s^{2}$ for every treatment group in every stratum:
  
<<>>=
d <- strat                    %>%
group_by(treatment, strata)   %>%
summarize(n = length(strata),    # Computing n
          s2 = var (x),          # Computing sample variances
          m = mean(x))           # Computing sample means
d
@
  
\noindent This allows us to group the remaining data set by stratum and calculate the sums and products given in the formulae:
  
<<>>=
d <- d                        %>%
group_by(strata)              %>%
mutate(sprod = s2*(n-1))      %>%
summarize(nsum = sum(n),          # Summing number of obs
          rnsum = sum(n) - 2,     # Subtracting 2
          ssum = sum(sprod),      # Summing the n-variance products
          nprod = prod(n),        # Multiplying the number of obs
          mdiff = m[1]-m[2])      # Computing the difference in means
@
\noindent The output of the above code is a tibble containing sums and products for every stratum, which is all we need to start assembling the test itself:
<<echo = FALSE>>=
  d
@
  
\noindent Now, what we need to do is calculate the weights and the estimate the variances for each stratum, we do this using the formulae given in the assignment, and the following code:
<<>>=
d <- d                                           %>%
mutate(weights = (nprod/nsum) / sum(nprod/nsum),       # Computing weights    
       sigma2 = (nsum/nprod) * (ssum/rnsum))     %>%   # Computing sigma2
select(mdiff, weights, sigma2)
@

\noindent Which yields the following tibble:

<<echo = FALSE>>=
d
@

\noindent As can be seen from the tibble printed above, we have managed to calculate the weights, estimators of the variances, and differences beteen means, which are the last components needed in order to calculate the t-statistic. We calculate the statistic like this:
  
<<>>=
d <- d                                        %>%
summarize(numerator = sum(weights * mdiff),
          denominator = sqrt(sum((weights^2) * sigma2)),
          t_stat = numerator/denominator,                # Test statistic
          stratified = TRUE)                  %>%        # Logical statement
select(t_stat, stratified)
d
@
  
\noindent And find the result given in a tibble showing the test statistic itself, and a logical statement indicating whether the statistic has been computed using the method for stratified data sets.

In the following section, we will present a method of calculating an ordinary two-sample t-test, which is a simpler form of what we have shown above.

\subsection{Classical t-test}
Again, we begin by simulating a data set we can use for our calculations:
  
<<>>=
set.seed(2018)
test <- tibble(x = c(rnorm(200, 25), rnorm(200, 45), rnorm(200, 75)),
               treatment = rep(1:2, 300))
@
  
\noindent Then we group our data by treatment only -- since there are no strata -- and carry on as we did in Section~\ref{sec:strat}. Hence, our code will not be broken up and commented as thoroughly as above. 
<<>>=
t <- test                           %>%
group_by(treatment)                 %>%
summarize(n = length(treatment),
          s2 = var(x),
          m = mean(x))              %>%
mutate(sprod = s2*(n-1))            %>%
summarize(nsum = sum(n),            # Summing number of obs
          rnsum = sum(n) - 2,       # Subtracting 2
          ssum = sum(sprod),        # Summing the n-variance products
          nprod = prod(n),          # Multiplying the number of obs
          mdiff = m[1]-m[2])        # Difference in means
@

\noindent This chunk of code will do the exact same thing as it did above, calculating $\bar{x}$, $n$, $s^{2}$ as well as the different sums and products needed to compute the test statistic. What is worth noting, however, is that in the following code the weights will be given as one, seeing as we have no strata to use:
  
<<>>=
t <- t %>%
mutate(weights = (nprod/nsum)/sum(nprod/nsum),  # Computing weights    
       sigma2 = (nsum/nprod)*(ssum/rnsum))      # Computing sigma2
t
@

\noindent And finally, we compute the t-statistic:
  
  <<>>=
  t <- t                              %>%
  select(mdiff, weights, sigma2)      %>%
  summarize(numerator = sum(weights * mdiff),
            denominator = sqrt(sum((weights^2) * sigma2)),
            t_stat = numerator / denominator,       # Test statistic
            stratified = FALSE)       %>%           # Logical statement
  select(t_stat, stratified)
@
  
\noindent Once again producing a tibble containing the test statistic and a logical statement. Notice, however, the logical statement reading \texttt{FALSE}, meaning that the stratified t-test has not been used.

<<echo = FALSE>>=
  t
@
  
\noindent Now, the only thing left to do is put everything together in a function. This will be done in Section~\ref{sec:tfunc}.

\subsection{Test Function}\label{sec:tfunc}
Finally, we turn to putting a \texttt{t\_test} function together. In order to save space and paper, we will denote the methods used to calculate the tests as \texttt{d} and \texttt{t}, just as we have done in previous sections. Instead we will focus on the composition of the function itself.

We begin by specifying an \texttt{if} statement, that makes sure the function will only accept input data in the form of a tibble or a data frame. If the function is called on any other type of data, it will return a warning message:
  
<<>>=
t_test <- function(data){
  if(is.tibble(data) & is.data.frame(data)){ # If statement limiting data
  } else {
    print("Data input needs to be a tibble or a data frame")
  } # Closes first if statement
}   # Closes function

@
  
\noindent Moreover, we want the function to look for a column called 'strata', and if there is no such column in the data, want it to perform a classical t-test rather than a stratified one:

\newpage
  
<<>>=
t_test <- function(data){
  if(is.tibble(data) & is.data.frame(data)){   # If statement limiting data
    if(any(colnames(data) == "strata")){       # Stratified test
      dstr <- d
      return(print.data.frame(dstr))
    } else {       # Classical test
      tcls <- t
      return(print.data.frame(tcls))
    }              # Closes test selection
  } else {
     print("Data input needs to be a tibble or a data frame")
  }                # Closes first if statement
}                  # Closes function

@
  
\noindent Finally, we create a vector of data just to try whether the if statements work. We also test the function using the data set we have previously simulated, namely \texttt{strat} and \texttt{test}:
  
<<>>=
vilgot <- 1:500 # Sample vector
t_test(strat)   # Stratified test
t_test(test)    # Classical test
t_test(vilgot)  # Test using vector input

@
  
\noindent So we see that the test function returns the values -- or messages -- we want it to. That concludes the writing of our function computing t-test, and thereby also this section of the paper.

\section{Presidential Election}
\subsection{Data Preparation}
In this task we were given three data files. We were asked to join them, clean them and then move on to  some analysis of the data. We begin by reading the data files:

<<warning = FALSE, message = FALSE>>=
crime_data <- read_tsv("crime_data.tsv")
dictionary_county_facts <- read_csv("county_facts_dictionary.csv")
county_facts <- read_csv("county_facts.csv")
results <- read_csv("general_result.csv")
@
  
After loading the data to \texttt{R} we want to merge the data into one data set. Each row is supposed to contain observations of a specific county, coded by the so called fips-code. First of all, we have to remove some rows in the county facts file since they are summaries for each state and the whole US. After that, we start off by joining the result file and the county facts file.

\newpage
<<message = FALSE>>=
results <- results %>%
arrange(combined_fips)                                             # Arranging the data by fips
colnames(results)[2] <- "fips"                                     # Changing column name
county_facts <- na.omit(county_facts, cols = "state_abbreviation") # Removing state summary rows
colnames(county_facts)[1] <- "fips"                                # Assigning matching column names
county_facts$fips <- as.character(county_facts$fips)               # Need for same class to join
results$fips <- as.character(results$fips)                         
results_county <- full_join(county_facts, results)                 # Joining data
@
  
In the crime file the fips-codes are separated into two columns, one with the state-code and one with the county-code. This causes some problems. To get the correct fips-code, we add one or two zeros between the state- and county codes. For a county with county-code 1 and state-code 1, we want the fips-code to be 1001. We solve this with a for loop based on logical arguments. After uniting the two columns into one fips-code column, we can join the crime data file with the rest of the data. We decide not to include the code in our report, but it can be found in the attached .R file.

<<include=FALSE>>=
  for (i in 1:nrow(crime_data)) {
    # If the county-code is les than 10, the state-code is multiplied by 100
    if(crime_data[i, 6] < 10){ 
      crime_data[i, 5] <- crime_data[i, 5]*100
    }else{
      # If the county-code is less than 10 but less than 100, the state-code is multiplied by 10
      if(crime_data[i, 6] < 100 & crime_data[i, 6] > 9){ 
        crime_data[i, 5] <- crime_data[i, 5]*10
      }
    }
  }

# Uniting the state- and county-code into one fips-code
crime_data <- unite(crime_data, FIPS_ST, FIPS_CTY, col = "fips", sep = "") 

# Joining results_county och crime_Data
sammanslaget <- full_join(results_county, crime_data)

# Moving columns of interest to the beginning of the file
sammanslaget <- sammanslaget %>%
  select(fips, per_gop_2016, total_votes_2016, everything()) 

@
  
Since we are interested in the results of the Republican party, we remove the rows which have missing values regarding the Republican results. In doing so, we end up with 3112 observations.

<<>>=
sammanslaget <- na.omit(sammanslaget, cols = "per_gop_2016") 
@
  
\noindent We also create a new variable, \texttt{gop\_win}, which takes on the value 1 if the Republicans got more votes than the Democrats

<<>>=
sammanslaget <- sammanslaget                    %>% 
mutate(gop_win = per_gop_2016 - per_dem_2016)   %>%  # Creating gop_win
select(gop_win, everything())

for (i in 1:nrow(sammanslaget)) { # gop_win is in first row
  if(sammanslaget[i, 1] > 0){     # Takes value 1 if GOP > Dems 
    sammanslaget[i, 1] <- 1
  } else {
    sammanslaget[i, 1] <- 0       # Takes value 0 if Dems > GOP
  }
}
sammanslaget$gop_win <- as.factor(sammanslaget$gop_win) # gop_win as factor variable
@
  
\subsection{Data analysis}
In this section, we focus on exploratory data analysis. This consists mostly of data visualisation and providing summaries of our data sets.

Figure \ref{fig:scatter1} shows the final result for the republican party explained by the median income. The best fit line shows that there is a negative relation between household income and support for the republican party. It seems republicans tend to have greater support in poor areas. Assuming wages tend to be higher in urban areas -- or along the coasts of the mainland -- this could suggest that a fair share of Trump's supporters are part of the rural population, seeing as the Republicans tend to do better in inland states.

Figure \ref{fig:histogram1} shows a comparison of Republican election results over the past two elections. As can be seen, the GOP managed to attain a qualified majority in more counties in 2016 than in 2012, indicating that the result was indeed pleasing from Trump's point of view. However, at first glance it is hard to tell whether the lower frequencies in the interval stretching from from about 40\% to around 60\% of the vote are compensated by the greater frequencies at higher percentages.

\newpage

<<scatter1, fig.width = 5.5, fig.asp = 0.62, fig.align = "center", fig.cap = "Scatterplot of republican election result and median income in that county", echo=FALSE, fig.pos="h!", message = FALSE>>=
ggplot(data = sammanslaget, 
       mapping = aes(x = INC110213, y = per_gop_2016,
                     color = HSD310213)) +
geom_point(size = 2) +
geom_smooth(method = lm, se = TRUE, color = "blue",
            size = 1.5) +
scale_y_continuous(breaks = c(0, 0.20, 0.40, 0.60, 0.80), 
                   labels = c("0%", "20%", "40%", "60%", "80%")) +
scale_x_continuous(breaks = c(25000, 50000, 75000, 100000, 125000), 
                   labels = c("$25000", "$50000", "$75000", "$100000", "$125000")) +
labs(title = "2016 US presidential election",
     subtitle = "GOP result and median income",
     x = "Median household income, 2009-2013",
     y = "Final result for the Republican party",
     color = "Persons /
household")
@
  
  
  <<histogram1, fig.width = 5.5, fig.asp = 0.62, fig.align = "center", fig.cap = "Histogram of republican elecetion result in 2016 and 2012", echo=FALSE, fig.pos ="h!", message = FALSE>>=
ggplot(data = sammanslaget) + 
geom_histogram(mapping = aes(x = per_gop_2016), 
               fill = "red", alpha = 1) +
geom_histogram(mapping = aes(x = per_gop_2012), 
               fill = "blue", alpha = 0.5) +
scale_x_continuous(breaks = c(0, 0.20, 0.40, 0.60, 0.80), 
                   labels = c("0%", "20%", "40%", "60%", "80%")) +
annotate(geom="text", x=0.93, y=287, label="2016", 
         color="red", size = 5) +
annotate(geom="text", x=0.93, y=267, label="2012", 
         color="blue", alpha = 0.5, size = 5) +
labs(title = "Histogram of GOP final results in 2016 and 2012",
     x = "Votes in percent",
     y = "Number of counties")

@
  
\newpage
\noindent Figure \ref{fig:boxplot1} shows two boxplots, one for the level of education in counties won by the democrats and one for the level of education in counties won by the Republicans. We can see that the level of education tends to be higher in counties won by the democrats.

<<boxplot1, fig.width = 6, fig.height =  7, fig.align = "center", fig.cap = "Boxplot of the level of education in counties won by each party.", echo=FALSE, message = FALSE, fig.pos = "h!">>=
ggplot(sammanslaget, aes(x = gop_win, y = EDU685213, fill = gop_win)) +
geom_boxplot() +
scale_y_continuous(breaks = c(0, 20, 40, 60, 80), 
                   labels = c("0%", "20%", "40%", "60%", "80%")) +
scale_x_discrete(breaks = c(0, 1), labels = c("Democrats", "Republicans")) +
labs(title = "2016 US presidential election",
     subtitle = "Boxplot of the educational level",
     x = "Winning party",
     y = "B.Sc. or higher, % of persons age 25+, 2009-13",
     fill = "Carried by") +
scale_fill_manual(labels = c("Dems", "GOP"),
                  values = c("blue", "red"))
@
  
\newpage

\noindent Figure \ref{fig:densityplot} shows the distribution of the share of elderly people in counties won by each party. We can see that the GOP generally won in counties where elderly people had a bigger share of the total population, perhaps indicating to Trump that his make America great again slogan worked well among those who are old enough to remember the "good old days".

<<densityplot,fig.width = 6, fig.asp = 0.62, fig.align = "center", fig.cap = "Distribution of the share of elderly, split by winning party", echo=FALSE>>=
  ggplot(sammanslaget, aes(x = AGE775214, 
                           fill = gop_win,
                           color = gop_win)) + 
  geom_density() +
  facet_wrap(~gop_win, nrow = 2) +
  labs(title = "Elderly votes in the 2016 Presidiential Election",
       x = " Persons aged 65 or over, %",
       y = "Density",
       fill = "Carried by") +
  guides(color = FALSE) + # Removing legend for "color"
  scale_fill_manual(labels = c("Dems", "GOP"), # Labelling "fill" legends
                    values = c("blue", "red")) + # Using party colours for fill
  scale_color_manual(values = c("blue", "red")) + # Using party colour for borders
  theme(strip.background = element_blank(), # Removing facet_wrap labels
        strip.text.x = element_blank())
@
  
  The regression line in Figure \ref{fig:coolscatter} shows a weak, negative relationship between the level of high school graduation and support for the Republican party. Moreover, it seems that counties that have a low rate of high school graduation also tend to have a larger number of people in each household. 
  
This graph concludes the exploratory data analysis, and in the next couple of sections we will produce conditional and unconditional summaries of the data.

\newpage

<<coolscatter, fig.height = 5, fig.aso = 0.62, fig.align = "center", fig.cap = "Scatterplot of republican result, educational level, income and persons per household", echo = FALSE, fig.pos = "h!">>=
  # Scatterplot of republican result, educational level, income and persons per household
ggplot(data = sammanslaget, aes(x = EDU635213, y = per_gop_2016, size = HSD310213, color = INC110213)) +
  geom_point() +
  geom_smooth(method = lm, se = TRUE, color = "blue", size = 1.5) +
  scale_y_continuous(breaks = c(0, 0.20, 0.40, 0.60, 0.80), 
                     labels = c("0%", "20%", "40%", "60%", "80%")) +
  scale_x_continuous(breaks = c(60, 80, 100), 
                     labels = c("60%", "80%", "100%")) +
  scale_color_gradient(low = "purple", high = "green") +
  labs(title = "2016 US presidential election",
       x = "Share of population with high school graduation",
       y = "Republican result",
       color = "Median income",
       size = "Persons / household")
@
  
  
  \subsection{Unconditional Summaries}
This section will provide three summaries in which the data has not been restricted by any conditions, meaning that there is no grouping or filtering whatsoever.

We begin with a table showing a summary of per capita money income over the past 12 months. All values are given in the 2013 value of the US Dollar, and the sample was taken during the period stretching from 2009 to 2013. We can see that the gap between the maximum and minimum incomes is strikingly big -- almost \$54,000 -- which might indeed be the reason why Trump seemed to do well among poorer voters.

<<table1, results = "asis", echo = FALSE>>=
  xtable(t(as.matrix(summary(sammanslaget$INC910213))), caption = "Summary of the variable per capita income, dollars.", label = "income")
@
  
  Table \ref{tab:murder} shows a summary of the variable that counts the arrests and offenses related to murder. It shows that the median number of arrests or offences is 0, meaning that in at least half of the counties, no such event had occured during the period studied. Perhaps, this could serve as an indicator that Trump tended to overdo his rhetoric regarding the crime rates, and the safety of most Americans.
\newpage

<<table2, results = "asis", echo = FALSE>>=
  xtable(t(as.matrix(summary(sammanslaget$MURDER))), caption = "Summary of the variable murder.",
         label = "tab:murder", table.placement = "h!")
@
  
 Table \ref{tab:pop} provides a brief summary of the 2014 estimate of the population. We see that more than half of the counties have an estimated population of less than 26000, meaning that there is greater room for an election to be decided by a very narrow margin of votes in each county.

<<table3, results = "asis", echo = FALSE>>=
  xtable(t(as.matrix(summary(sammanslaget$PST045214))), caption = "Summary of the variable population.", 
         label = "tab:pop")
@
  
  \subsection{Grouped Summaries}

<<include = FALSE>>=
  tab1 <- sammanslaget %>%
  select(gop_win, PST045214, ROBBERY, SBO315207, HSD310213, VET605213) %>%
  mutate(mrb = ROBBERY/PST045214,         # Robberies per person
         mvet = VET605213/PST045214,
         GOP = gop_win) %>%  # Veterans per person
  group_by(GOP) %>%
  summarize("Robberies" = sum(ROBBERY),          # Number of robberies
            "Avg. robberies" = mean(mrb),             # Mean number of robs/person
            "Black Firms" = mean(SBO315207),  # Mean share of black firms
            "Ppl per hshld" = mean(HSD310213),      # Average number of peeps per hhld
            "Share Veterans" = mean(mvet)) %>%
  select(GOP,"Robberies", "Avg. robberies","Black Firms",
         "Ppl per hshld", "Share Veterans")
@
  
  \noindent We beign the grouped summaries with a closer look at the differences between states carried by Republicans and states carried by Democrats. In Table \ref{tab:fsum}, if GOP = 1, it means Trump carried a majority of the votes in more than 50\% of the counties in a particular state.

<<"fsum",results = "asis", echo = FALSE>>=
  xtable(tab1, digits = c(0,0,3,5,3,3,5), caption = "Variables grouped by 'carrying' party", label = "tab:fsum")
@
  
\noindent The table shows that people living in states "carried" by Democrats run a greater risk of getting robbed, and that the states themselves have a higher rate of firms run by blacks, and a slightly higher number of people living in each household. However, states "carried" by the GOP have a greater share of veterans among their inhabitants.

In Figure \ref{fig:wisp} we have plotted the mean number of firms against the share of women in the population, grouped by state. This means that every point in the scatterplot represents an individual state, as is shown by the labels attached to each point. The graph suggests there is no relationship between the two variables.
\newpage

<<"wisp", echo = FALSE, message = FALSE, fig.align = "center", fig.width = 6, fig.asp = 0.62, fig.cap = "Pct women plotted against mean no. of firms, by state", fig.pos = "h!">>=
sammanslaget %>%
group_by(state_abbreviation) %>%
select(SEX255214, SBO001207, RHI725214) %>%
summarize(wmn = mean(SEX255214),     # Pct wmn
mfrms = mean(SBO001207),   # Avg no. firms pr county
hisp = mean(RHI725214))%>% # Pct hispanics
ggplot(mapping = aes(y = mfrms, x = wmn)) +
geom_point() +
geom_label_repel(aes(label = state_abbreviation),
box.padding   = 0.35, 
point.padding = 0.5,
segment.color = 'blue') +
labs(title = "Women, states and firms",
x = "% Women",
y = "Mean number of firms per state") +
theme_classic()
@

\end{document}