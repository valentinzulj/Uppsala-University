\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[a4paper, total={6.1in, 8.1in}]{geometry}

\author{Valentin Zulj \&  Vilgot \"{O}sterlund}
\title{Solutions to Assignment 2}
\date{October 10, 2018}

\begin{document}
\maketitle

<<include=FALSE>>=
library(tidyverse)
library(MASS)
library(glmnet)
@

\section{Problem 2}
In this task, we were asked to evaluate seven different models regarding their perfomance in predicting house prices. To do this we have a data file with 2930 observations of sold houses, where 1465 observations correspond to the training data and the remaining 1465 observations correspond to the test data. In addition to sale price, the data file includes 20 variables ment to explain the price. The layup is to create the different models on the training data and then predict the sale prices in the test data. The models are evaluated on the root mean square error (RMSE) of their predictions. To start off, we import the data and separate the data file into a training- and test set. We also create a table were we will have the RMSE for each model.

<<>>=
house <- read.csv("house_prices.csv")     # Importing the data
train <- house %>%                        # Training set
  filter(train == TRUE)
test <- house %>%                         # Test set
  filter(train == FALSE)
train <- train[1:21]                      # Removing column train
test <- test[1:21]
models_rmse <- matrix(nrow = 1, ncol = 7) # Preparing a table
models_rmse <- as.data.frame(models_rmse)
colnames(models_rmse) <- 
  c("lr_all", "lr_step", "ridge", "LASSO", 
    "bagging", "feature bagging", "stacking")
@

The first model is a linear regression model with all 20 variables included. The RMSE for this model is 35 667.

<<warning=FALSE>>=
lr_all <- lm(SalePrice ~ ., data = train) # Linear model
lr_all_pred <- predict.lm(lr_all, 
        test, type = "response")          # Predicting test set
lr_all_mse <- 
  (test$SalePrice - lr_all_pred)^2        # Squared error
print(models_rmse[1, 1] <- 
  sqrt(mean(lr_all_mse)))                 # RMSE for lr_all
@

The next model is also a linear regression model, but the included variables have been chosen using the step-wise procedure in the function \texttt{step()}. The direction of the step-wise search have been set to both. The RMSE for the model given by this method is 35 609, so just a little bit better than with all variables included.

<<warning=FALSE>>=
lr_step <- step(lr_all,                   # R choose variables
      direction = "both", trace = FALSE)      
lr_step_pred <- predict.lm(lr_step, 
        test, type = "response")          # Predicting test set
lr_step_mse <- 
  (test$SalePrice - lr_step_pred)^2       # Squared error
print(models_rmse[1, 2] <- 
        sqrt(mean(lr_step_mse)))          # RMSE for lr_step
@

We move on to penalized regression, starting with a ridge regression model. The value of lambda decides how much the coefficients are beeing penalized. We find the best value of lambda by cross validation on the training set. The value of lambda that minimizes the mean square error (MSE) in the cross validation is chosen as the lambda in the prediction of the test set. The RMSE of this model is 34 322, an improvement compared to the previous models.

<<>>=
set.seed(1)
x <- model.matrix(SalePrice ~ ., train)   # Preparing the training set
y <- train$SalePrice                      # Response variable
xtest <- 
  model.matrix(SalePrice ~ ., test)       # Preparing the test set
ytest <- test$SalePrice                   # Response variable to predict
cv_out <- cv.glmnet(x, y, alpha = 0)      # CV, ridge
ridge_pred <- predict(cv_out,             # Predicting, lambda = value
        xtest, lambda = cv_out$cv.min)    # that minimize MSE from CV
ridge <- (test$SalePrice - ridge_pred)^2  # Squared error
print(models_rmse[1, 3] <- 
        sqrt(mean(ridge)))                # RMSE for ridge
@

The next model is LASSO, which is similar to ridge regression but with the difference that it can impel the coefficients to be zero. The value of lambda is found by the same procedure as in ridge regression. The RMSE of the LASSO model is 35 127, which puts it on second place when it comes to prediction accuracy.

<<>>=
set.seed(1)
cv_out_lasso <- 
  cv.glmnet(x, y, alpha = 1)              # Cross validation, LASSO
lasso_pred <- predict(cv_out_lasso,       # Predicting, lambda = value
  xtest, lambda = cv_out_lasso$cv.min)    # that minimize MSE from CV
lasso <- (ytest - lasso_pred)^2           # Squared error
print(models_rmse[1, 4] <- 
  sqrt(mean(lasso)))                      # RMSE for LASSO
@

Last up, we have stacking, which can be seen as a multi stage rocket for prediction. In the first stage, we use leave one out cross validation (LOOCV) to predict the prices in the training set. This yields four predictions for each observation. In the next stage, we regress the true values on the predicted values. This gives us a linear model, here called \texttt{reg\_y}. Now we use the models achieved earlier (\texttt{lr\_all, lr\_step, ridge} and \texttt{LASSO}) to predict the sale prices in the training set. Again, this gives us four predictions for each observation. In the last stage, we use the theese values to predict the sale prices with the linear model \texttt{reg\_y}. This turned out to be the best model for predicting the sale prices in the test set, with a RMSE of 34 783.

<<warning = FALSE>>=
n <- nrow(train)
hat <- matrix(NA, nrow = n, ncol = 4)
x <- model.matrix(SalePrice ~ ., train)   # Preparing the training set
x_test <- 
  model.matrix(SalePrice ~ ., test)       # Preparing test set
y <- train$SalePrice 
cv_las <- cv.glmnet(x, y, alpha = 1)      # LASSO model
cv_rid <- cv.glmnet(x, y, alpha = 0)      # Ridge model
mod_step <- 
  step(lm(SalePrice ~ ., 
          data = train), trace = FALSE)   # Step model
for(i in 1:n){
  tr <- train[-i, ]                       # LOOCV
  te <- train[i, ]
  x_tr <- 
    model.matrix(SalePrice ~ ., tr)       
  y_tr <- tr$SalePrice 
  x1 <- model.matrix(SalePrice ~ ., te)
  l_mod <- lm(SalePrice ~ ., data = tr)
  las_mod <- 
    glmnet(x_tr, y_tr, alpha = 1)
  rid_mod <- 
    glmnet(x_tr, y_tr, alpha = 0)
  st_mod <- 
    lm(formula(mod_step), data = tr)
  options(warn = -1)
  hat[i, 1] <- predict(l_mod, te)        # Lr prediction
  hat[i, 2] <- predict(las_mod, x1, 
              s = cv_las$lambda.min, 
              type = "response")         # Lasso prediction
  hat[i, 3] <- predict(rid_mod, x1,
              s = cv_rid$lambda.min, 
              type = "response")         # Ridge prediction
  hat[i, 4] <- predict(st_mod, te)       # Step prediction
  options(warn = 1)}

reg_y <- lm(y ~ hat)                     # Regressing predictions
b <- 
  matrix(reg_y$coefficients, ncol = 1)   # Saving coefficients

options(warn = -1)
test_lm <- matrix(predict
   (lm(SalePrice ~ ., data = train), 
     test), ncol = 1)                    # Predicting lr
test_step <- matrix(predict
        (mod_step, test), ncol = 1)      # Predicting step
test_lass <- matrix(
  predict(glmnet(x_tr, y_tr, alpha = 1),
          x_test, s = cv_las$lambda.min, 
          type = "response"), ncol = 1)  # Predicting LAsso
test_rid <- matrix(
  predict(glmnet(x_tr, y_tr, alpha = 1),
          x_test, s = cv_rid$lambda.min, 
          type = "response"), ncol = 1)  # Predicting Ridge
options(warn = 1)

intercept <- matrix(1, nrow = 1465)

predictions <- cbind(intercept, 
      test_lm, test_lass, 
      test_rid, test_step)               # Column bind predictions

stack_pred <- predictions %*% b          # Multiplying with coeffs
y_test <- test %>%
  pull(SalePrice)

stack <- tibble(response = y_test,
  predictions = as.numeric(stack_pred))  # Binding true and predicted

stack <- stack %>%
  mutate(dev_square = 
      (response - predictions)^2) %>%
  summarize(RMSE = 
      sqrt(mean(dev_square)))            # RMSE
print(models_rmse[1, 7] <- stack)

@


\end{document}
